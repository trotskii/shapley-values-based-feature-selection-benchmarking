{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_score\n",
    "import sys \n",
    "from tqdm.notebook import tqdm\n",
    "import sklearn.feature_extraction.text as ft \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../')\n",
    "import src.preprocessing.text_preprocessing as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['text.usetex'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = 'brown'\n",
    "base_path = '../results/'\n",
    "path = f'{base_path}{FOLDER}/'\n",
    "full_paths = [f'{path}{file}' for file in os.listdir(path)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "for p in full_paths:\n",
    "    name = p.split('/')[-1]\n",
    "    split = name.split('_')\n",
    "    method = '_'.join(split[1:-1])\n",
    "    n_words = split[-1].split('.')[0]\n",
    "    with open(p, 'r') as file:\n",
    "        if method not in files:\n",
    "            files[method] = {}\n",
    "        files[method][n_words] = json.load(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.620253164556962,\n",
       "  'recall': 0.5632183908045977,\n",
       "  'f1-score': 0.5903614457831324,\n",
       "  'support': 87},\n",
       " '1': {'precision': 0.4030612244897959,\n",
       "  'recall': 0.5563380281690141,\n",
       "  'f1-score': 0.4674556213017752,\n",
       "  'support': 142},\n",
       " '2': {'precision': 0.4461538461538462,\n",
       "  'recall': 0.47540983606557374,\n",
       "  'f1-score': 0.4603174603174603,\n",
       "  'support': 61},\n",
       " '3': {'precision': 0.5151515151515151,\n",
       "  'recall': 0.5666666666666667,\n",
       "  'f1-score': 0.5396825396825397,\n",
       "  'support': 90},\n",
       " '4': {'precision': 0.7123287671232876,\n",
       "  'recall': 0.6753246753246753,\n",
       "  'f1-score': 0.6933333333333332,\n",
       "  'support': 77},\n",
       " '5': {'precision': 0.7052631578947368,\n",
       "  'recall': 0.6568627450980392,\n",
       "  'f1-score': 0.6802030456852792,\n",
       "  'support': 102},\n",
       " '6': {'precision': 0.75,\n",
       "  'recall': 0.13636363636363635,\n",
       "  'f1-score': 0.23076923076923075,\n",
       "  'support': 22},\n",
       " '7': {'precision': 0.6879432624113475,\n",
       "  'recall': 0.6217948717948718,\n",
       "  'f1-score': 0.6531986531986532,\n",
       "  'support': 156},\n",
       " '8': {'precision': 0.42857142857142855,\n",
       "  'recall': 0.4017857142857143,\n",
       "  'f1-score': 0.4147465437788019,\n",
       "  'support': 112},\n",
       " '9': {'precision': 0.6190476190476191,\n",
       "  'recall': 0.611764705882353,\n",
       "  'f1-score': 0.6153846153846154,\n",
       "  'support': 85},\n",
       " '10': {'precision': 0.6701030927835051,\n",
       "  'recall': 0.7558139534883721,\n",
       "  'f1-score': 0.7103825136612022,\n",
       "  'support': 86},\n",
       " '11': {'precision': 0.5416666666666666,\n",
       "  'recall': 0.3333333333333333,\n",
       "  'f1-score': 0.4126984126984126,\n",
       "  'support': 39},\n",
       " '12': {'precision': 0.5263157894736842,\n",
       "  'recall': 0.2564102564102564,\n",
       "  'f1-score': 0.3448275862068965,\n",
       "  'support': 39},\n",
       " '13': {'precision': 0.4807692307692308,\n",
       "  'recall': 0.6578947368421053,\n",
       "  'f1-score': 0.5555555555555556,\n",
       "  'support': 76},\n",
       " '14': {'precision': 0.8571428571428571,\n",
       "  'recall': 0.3333333333333333,\n",
       "  'f1-score': 0.48,\n",
       "  'support': 18},\n",
       " 'accuracy': 0.5604026845637584,\n",
       " 'macro avg': {'precision': 0.5975847748157656,\n",
       "  'recall': 0.5068209922575028,\n",
       "  'f1-score': 0.5232611038237925,\n",
       "  'support': 1192},\n",
       " 'weighted avg': {'precision': 0.5765497555686996,\n",
       "  'recall': 0.5604026845637584,\n",
       "  'f1-score': 0.5573759777907442,\n",
       "  'support': 1192}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files['shap']['1000']['classification_report_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "df = pd.read_csv('../data/brown_corpus/brown_corpus.csv', sep=';')\n",
    "df = df.fillna('')\n",
    "df = df.astype('str')\n",
    "df['Label'] = df['Label'].astype('category')\n",
    "df['Label'] = df['Label'].cat.codes\n",
    "df['Text'] = df['Text'].apply(tp.normalize_text)\n",
    "\n",
    "count_vectorizer = ft.CountVectorizer()\n",
    "count_vectorizer.fit(df['Text'])\n",
    "vocabulary = count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extractor_timings(files: dict) -> pd.DataFrame:\n",
    "    methods = files.keys()\n",
    "    df = pd.DataFrame(columns=methods)\n",
    "    n_words_list = set()\n",
    "    method_list = set()\n",
    "    for method, words_dict in files.items():\n",
    "        method_list.add(method)\n",
    "        for n_words, info in words_dict.items():\n",
    "                df.loc[n_words, method] = info['timing']['extractor_fit']\n",
    "                n_words_list.add(n_words)\n",
    "                \n",
    "    df.index = df.index.astype('int')\n",
    "    df = df.sort_index()\n",
    "    df = df.apply(pd.to_timedelta)\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].dt.total_seconds()\n",
    "    return df, method_list, n_words_list\n",
    "\n",
    "def plot_extractors_timings(df) -> plt.figure:\n",
    "    axes = df.plot(logy=True, logx=True, figsize=(16, 10))\n",
    "    axes.set_xlabel('# selected words')\n",
    "    axes.set_ylabel('runtime (sec)')\n",
    "    axes.set_label('runtime (sec)')\n",
    "    plt.title('Feature extractor runtimes vs number of the selected words.')\n",
    "    plt.savefig('../figures/brown_extractor_runtime_vs_n_words.png')\n",
    "    plt.close()\n",
    "\n",
    "def get_selected_words_per_extractor_per_n_words(files: dict, vocabulary, n_words_list, method_list):\n",
    "    df_dict = {}\n",
    "\n",
    "    for n_words in n_words_list:\n",
    "        df_dict[n_words] = pd.DataFrame(index=vocabulary, columns=list(method_list)).fillna(0)\n",
    "\n",
    "    for method, words_dict in files.items():\n",
    "        for n_words, info in words_dict.items():\n",
    "            df_dict[n_words].loc[info['selected_vocabulary'], method] = 1\n",
    "    return df_dict\n",
    "\n",
    "def get_cross_jaccard_score(df):\n",
    "    methods = df.columns.tolist()\n",
    "    jaccard_df = pd.DataFrame(index=methods, columns=methods)\n",
    "    for method_1 in methods:\n",
    "        for method_2 in methods:\n",
    "            jaccard_df.loc[method_1, method_2] = jaccard_score(df[method_1], df[method_2])\n",
    "    return jaccard_df\n",
    "\n",
    "def get_similarity_metrics(df_dict):\n",
    "    correlations_dict = {}\n",
    "    jaccard_score_dict = {}\n",
    "    for n_words, df in df_dict.items():\n",
    "        df_filtered = df.loc[:,(df.sum(axis=0) != 0).values] # remove lfs (or other methods) when they have no values\n",
    "        correlations_dict[n_words] = df_filtered.corr() \n",
    "        jaccard_score_dict[n_words] = get_cross_jaccard_score(df_filtered)\n",
    "    \n",
    "    return correlations_dict, jaccard_score_dict\n",
    "\n",
    "def compare_shap_over_n_words_set_similarity(df_dict: dict, n_words_list, method_list):\n",
    "    df_comp = pd.DataFrame(columns=method_list, index=n_words_list)\n",
    "    for n_words, df in df_dict.items():\n",
    "        cols = df.columns\n",
    "        df_comp.loc[n_words, cols] = df['shap'][cols]\n",
    "    df_comp.index = df_comp.index.astype('int')\n",
    "    df_comp = df_comp.sort_index()\n",
    "    df_comp = df_comp.drop(columns=['shap'])\n",
    "    return df_comp\n",
    "\n",
    "def compare_performance_over_n_words_enron(files, n_words_list, method_list, baseline=None):\n",
    "    metrics = ['precision', 'recall', 'f1-score']\n",
    "    cols = []\n",
    "    for method in method_list:\n",
    "        for metric in metrics:\n",
    "            cols.append((method, metric))\n",
    "    df_metrics = pd.DataFrame(columns=pd.MultiIndex.from_tuples(cols), index=n_words_list)\n",
    "\n",
    "    for method, words_dict in files.items():\n",
    "        for n_words, info in words_dict.items():\n",
    "            for metric in metrics:\n",
    "                df_metrics.loc[n_words, (method, metric)] = info['classification_report_test']['weighted avg'][metric]\n",
    "    if baseline is not None:\n",
    "        for n_words in n_words_list:\n",
    "            for metric in metrics:\n",
    "                df_metrics.loc[n_words, ('baseline',metric)] = baseline['weighted avg'][metric]\n",
    "    \n",
    "    df_metrics.index = df_metrics.index.astype('int')\n",
    "    df_metrics = df_metrics.sort_index()\n",
    "    return df_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, method_list, n_words_list = get_extractor_timings(files)\n",
    "df.to_csv('../results/tables/brown_n_word_timings.csv', sep=';')\n",
    "plot_extractors_timings(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = get_selected_words_per_extractor_per_n_words(files, vocabulary, n_words_list, method_list)\n",
    "correlations_dict, jaccard_score_dict = get_similarity_metrics(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_correlations = compare_shap_over_n_words_set_similarity(correlations_dict, n_words_list, method_list)\n",
    "shap_jaccard = compare_shap_over_n_words_set_similarity(jaccard_score_dict, n_words_list, method_list)\n",
    "shap_correlations.to_csv('../results/tables/brown_correlations.csv', sep=';')\n",
    "shap_jaccard.to_csv('../results/tables/brown_jaccard.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/brown_baseline.json', 'r') as file:\n",
    "    baseline_brown = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = compare_performance_over_n_words_enron(files, n_words_list, method_list, baseline=baseline_brown['classification_report_test']) \n",
    "df_metrics.to_csv('../results/tables/brown_performance.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">chi2</th>\n",
       "      <th colspan=\"3\" halign=\"left\">eccd</th>\n",
       "      <th colspan=\"3\" halign=\"left\">term_strength</th>\n",
       "      <th>mutual_information</th>\n",
       "      <th>...</th>\n",
       "      <th>trl</th>\n",
       "      <th colspan=\"3\" halign=\"left\">tfidf</th>\n",
       "      <th colspan=\"3\" halign=\"left\">shap</th>\n",
       "      <th colspan=\"3\" halign=\"left\">baseline</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>...</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.093401</td>\n",
       "      <td>0.103188</td>\n",
       "      <td>0.081946</td>\n",
       "      <td>0.08652</td>\n",
       "      <td>0.138423</td>\n",
       "      <td>0.052191</td>\n",
       "      <td>0.15577</td>\n",
       "      <td>0.122483</td>\n",
       "      <td>0.116472</td>\n",
       "      <td>0.240846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051524</td>\n",
       "      <td>0.235641</td>\n",
       "      <td>0.145134</td>\n",
       "      <td>0.107318</td>\n",
       "      <td>0.22341</td>\n",
       "      <td>0.213087</td>\n",
       "      <td>0.205448</td>\n",
       "      <td>0.718198</td>\n",
       "      <td>0.692953</td>\n",
       "      <td>0.68397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.327815</td>\n",
       "      <td>0.312081</td>\n",
       "      <td>0.314942</td>\n",
       "      <td>0.286137</td>\n",
       "      <td>0.194631</td>\n",
       "      <td>0.167472</td>\n",
       "      <td>0.335594</td>\n",
       "      <td>0.29698</td>\n",
       "      <td>0.308667</td>\n",
       "      <td>0.335952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113259</td>\n",
       "      <td>0.352469</td>\n",
       "      <td>0.312919</td>\n",
       "      <td>0.313933</td>\n",
       "      <td>0.333439</td>\n",
       "      <td>0.311242</td>\n",
       "      <td>0.317682</td>\n",
       "      <td>0.718198</td>\n",
       "      <td>0.692953</td>\n",
       "      <td>0.68397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.399376</td>\n",
       "      <td>0.386745</td>\n",
       "      <td>0.387908</td>\n",
       "      <td>0.379083</td>\n",
       "      <td>0.260067</td>\n",
       "      <td>0.259112</td>\n",
       "      <td>0.360713</td>\n",
       "      <td>0.328859</td>\n",
       "      <td>0.338548</td>\n",
       "      <td>0.349741</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213319</td>\n",
       "      <td>0.351809</td>\n",
       "      <td>0.348993</td>\n",
       "      <td>0.34758</td>\n",
       "      <td>0.378142</td>\n",
       "      <td>0.364094</td>\n",
       "      <td>0.367382</td>\n",
       "      <td>0.718198</td>\n",
       "      <td>0.692953</td>\n",
       "      <td>0.68397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.421726</td>\n",
       "      <td>0.408557</td>\n",
       "      <td>0.412039</td>\n",
       "      <td>0.413566</td>\n",
       "      <td>0.410235</td>\n",
       "      <td>0.406668</td>\n",
       "      <td>0.413927</td>\n",
       "      <td>0.395973</td>\n",
       "      <td>0.402467</td>\n",
       "      <td>0.399631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367004</td>\n",
       "      <td>0.419435</td>\n",
       "      <td>0.407718</td>\n",
       "      <td>0.410236</td>\n",
       "      <td>0.429727</td>\n",
       "      <td>0.417785</td>\n",
       "      <td>0.422438</td>\n",
       "      <td>0.718198</td>\n",
       "      <td>0.692953</td>\n",
       "      <td>0.68397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.516304</td>\n",
       "      <td>0.500839</td>\n",
       "      <td>0.503248</td>\n",
       "      <td>0.517232</td>\n",
       "      <td>0.510906</td>\n",
       "      <td>0.510158</td>\n",
       "      <td>0.465611</td>\n",
       "      <td>0.458054</td>\n",
       "      <td>0.459769</td>\n",
       "      <td>0.49909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.39464</td>\n",
       "      <td>0.504958</td>\n",
       "      <td>0.487416</td>\n",
       "      <td>0.490759</td>\n",
       "      <td>0.506378</td>\n",
       "      <td>0.493289</td>\n",
       "      <td>0.493926</td>\n",
       "      <td>0.718198</td>\n",
       "      <td>0.692953</td>\n",
       "      <td>0.68397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.599997</td>\n",
       "      <td>0.593121</td>\n",
       "      <td>0.592533</td>\n",
       "      <td>0.599241</td>\n",
       "      <td>0.592282</td>\n",
       "      <td>0.592301</td>\n",
       "      <td>0.527559</td>\n",
       "      <td>0.502517</td>\n",
       "      <td>0.508079</td>\n",
       "      <td>0.547545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.373913</td>\n",
       "      <td>0.570023</td>\n",
       "      <td>0.548658</td>\n",
       "      <td>0.55032</td>\n",
       "      <td>0.57655</td>\n",
       "      <td>0.560403</td>\n",
       "      <td>0.557376</td>\n",
       "      <td>0.718198</td>\n",
       "      <td>0.692953</td>\n",
       "      <td>0.68397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>0.649813</td>\n",
       "      <td>0.63255</td>\n",
       "      <td>0.634677</td>\n",
       "      <td>0.687217</td>\n",
       "      <td>0.678691</td>\n",
       "      <td>0.679113</td>\n",
       "      <td>0.628443</td>\n",
       "      <td>0.60906</td>\n",
       "      <td>0.608692</td>\n",
       "      <td>0.609246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528577</td>\n",
       "      <td>0.649043</td>\n",
       "      <td>0.637584</td>\n",
       "      <td>0.635232</td>\n",
       "      <td>0.667415</td>\n",
       "      <td>0.657718</td>\n",
       "      <td>0.653321</td>\n",
       "      <td>0.718198</td>\n",
       "      <td>0.692953</td>\n",
       "      <td>0.68397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>0.677005</td>\n",
       "      <td>0.665268</td>\n",
       "      <td>0.662692</td>\n",
       "      <td>0.695175</td>\n",
       "      <td>0.687081</td>\n",
       "      <td>0.684728</td>\n",
       "      <td>0.671095</td>\n",
       "      <td>0.656879</td>\n",
       "      <td>0.653155</td>\n",
       "      <td>0.689206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496224</td>\n",
       "      <td>0.66554</td>\n",
       "      <td>0.644295</td>\n",
       "      <td>0.639693</td>\n",
       "      <td>0.645298</td>\n",
       "      <td>0.638423</td>\n",
       "      <td>0.630951</td>\n",
       "      <td>0.718198</td>\n",
       "      <td>0.692953</td>\n",
       "      <td>0.68397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>0.684984</td>\n",
       "      <td>0.678691</td>\n",
       "      <td>0.668407</td>\n",
       "      <td>0.709728</td>\n",
       "      <td>0.697148</td>\n",
       "      <td>0.691344</td>\n",
       "      <td>0.688091</td>\n",
       "      <td>0.671141</td>\n",
       "      <td>0.664329</td>\n",
       "      <td>0.690701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546121</td>\n",
       "      <td>0.700859</td>\n",
       "      <td>0.687919</td>\n",
       "      <td>0.680537</td>\n",
       "      <td>0.669754</td>\n",
       "      <td>0.651846</td>\n",
       "      <td>0.644496</td>\n",
       "      <td>0.718198</td>\n",
       "      <td>0.692953</td>\n",
       "      <td>0.68397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>0.704048</td>\n",
       "      <td>0.685403</td>\n",
       "      <td>0.680983</td>\n",
       "      <td>0.709212</td>\n",
       "      <td>0.69547</td>\n",
       "      <td>0.691205</td>\n",
       "      <td>0.713322</td>\n",
       "      <td>0.689597</td>\n",
       "      <td>0.682676</td>\n",
       "      <td>0.724977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625048</td>\n",
       "      <td>0.702355</td>\n",
       "      <td>0.676174</td>\n",
       "      <td>0.670141</td>\n",
       "      <td>0.673435</td>\n",
       "      <td>0.644295</td>\n",
       "      <td>0.63926</td>\n",
       "      <td>0.718198</td>\n",
       "      <td>0.692953</td>\n",
       "      <td>0.68397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>0.699836</td>\n",
       "      <td>0.682047</td>\n",
       "      <td>0.674357</td>\n",
       "      <td>0.688619</td>\n",
       "      <td>0.677013</td>\n",
       "      <td>0.670421</td>\n",
       "      <td>0.699876</td>\n",
       "      <td>0.675336</td>\n",
       "      <td>0.670353</td>\n",
       "      <td>0.713825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.639912</td>\n",
       "      <td>0.713236</td>\n",
       "      <td>0.687081</td>\n",
       "      <td>0.683315</td>\n",
       "      <td>0.67606</td>\n",
       "      <td>0.653523</td>\n",
       "      <td>0.645184</td>\n",
       "      <td>0.718198</td>\n",
       "      <td>0.692953</td>\n",
       "      <td>0.68397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           chi2                          eccd                      \\\n",
       "      precision    recall  f1-score precision    recall  f1-score   \n",
       "10     0.093401  0.103188  0.081946   0.08652  0.138423  0.052191   \n",
       "50     0.327815  0.312081  0.314942  0.286137  0.194631  0.167472   \n",
       "100    0.399376  0.386745  0.387908  0.379083  0.260067  0.259112   \n",
       "200    0.421726  0.408557  0.412039  0.413566  0.410235  0.406668   \n",
       "500    0.516304  0.500839  0.503248  0.517232  0.510906  0.510158   \n",
       "1000   0.599997  0.593121  0.592533  0.599241  0.592282  0.592301   \n",
       "3000   0.649813   0.63255  0.634677  0.687217  0.678691  0.679113   \n",
       "5000   0.677005  0.665268  0.662692  0.695175  0.687081  0.684728   \n",
       "10000  0.684984  0.678691  0.668407  0.709728  0.697148  0.691344   \n",
       "15000  0.704048  0.685403  0.680983  0.709212   0.69547  0.691205   \n",
       "25000  0.699836  0.682047  0.674357  0.688619  0.677013  0.670421   \n",
       "\n",
       "      term_strength                     mutual_information  ...       trl  \\\n",
       "          precision    recall  f1-score          precision  ...  f1-score   \n",
       "10          0.15577  0.122483  0.116472           0.240846  ...  0.051524   \n",
       "50         0.335594   0.29698  0.308667           0.335952  ...  0.113259   \n",
       "100        0.360713  0.328859  0.338548           0.349741  ...  0.213319   \n",
       "200        0.413927  0.395973  0.402467           0.399631  ...  0.367004   \n",
       "500        0.465611  0.458054  0.459769            0.49909  ...   0.39464   \n",
       "1000       0.527559  0.502517  0.508079           0.547545  ...  0.373913   \n",
       "3000       0.628443   0.60906  0.608692           0.609246  ...  0.528577   \n",
       "5000       0.671095  0.656879  0.653155           0.689206  ...  0.496224   \n",
       "10000      0.688091  0.671141  0.664329           0.690701  ...  0.546121   \n",
       "15000      0.713322  0.689597  0.682676           0.724977  ...  0.625048   \n",
       "25000      0.699876  0.675336  0.670353           0.713825  ...  0.639912   \n",
       "\n",
       "          tfidf                          shap                      baseline  \\\n",
       "      precision    recall  f1-score precision    recall  f1-score precision   \n",
       "10     0.235641  0.145134  0.107318   0.22341  0.213087  0.205448  0.718198   \n",
       "50     0.352469  0.312919  0.313933  0.333439  0.311242  0.317682  0.718198   \n",
       "100    0.351809  0.348993   0.34758  0.378142  0.364094  0.367382  0.718198   \n",
       "200    0.419435  0.407718  0.410236  0.429727  0.417785  0.422438  0.718198   \n",
       "500    0.504958  0.487416  0.490759  0.506378  0.493289  0.493926  0.718198   \n",
       "1000   0.570023  0.548658   0.55032   0.57655  0.560403  0.557376  0.718198   \n",
       "3000   0.649043  0.637584  0.635232  0.667415  0.657718  0.653321  0.718198   \n",
       "5000    0.66554  0.644295  0.639693  0.645298  0.638423  0.630951  0.718198   \n",
       "10000  0.700859  0.687919  0.680537  0.669754  0.651846  0.644496  0.718198   \n",
       "15000  0.702355  0.676174  0.670141  0.673435  0.644295   0.63926  0.718198   \n",
       "25000  0.713236  0.687081  0.683315   0.67606  0.653523  0.645184  0.718198   \n",
       "\n",
       "                          \n",
       "         recall f1-score  \n",
       "10     0.692953  0.68397  \n",
       "50     0.692953  0.68397  \n",
       "100    0.692953  0.68397  \n",
       "200    0.692953  0.68397  \n",
       "500    0.692953  0.68397  \n",
       "1000   0.692953  0.68397  \n",
       "3000   0.692953  0.68397  \n",
       "5000   0.692953  0.68397  \n",
       "10000  0.692953  0.68397  \n",
       "15000  0.692953  0.68397  \n",
       "25000  0.692953  0.68397  \n",
       "\n",
       "[11 rows x 27 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
