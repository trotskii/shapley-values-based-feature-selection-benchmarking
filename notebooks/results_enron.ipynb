{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_score\n",
    "import sys \n",
    "from tqdm.notebook import tqdm\n",
    "import sklearn.feature_extraction.text as ft \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../')\n",
    "import src.preprocessing.text_preprocessing as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['text.usetex'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = 'enron copy'\n",
    "base_path = '../results/'\n",
    "path = f'{base_path}{FOLDER}/'\n",
    "full_paths = [f'{path}{file}' for file in os.listdir(path)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "for p in full_paths:\n",
    "    name = p.split('/')[-1]\n",
    "    split = name.split('_')\n",
    "    method = '_'.join(split[1:-1])\n",
    "    n_words = split[-1].split('.')[0]\n",
    "    with open(p, 'r') as file:\n",
    "        if method not in files:\n",
    "            files[method] = {}\n",
    "        files[method][n_words] = json.load(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c60d94288a974a9f859b6c6c72abd233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33716 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137481\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df = pd.read_csv('../data/enron/enron_spam_data.csv', sep=',')\n",
    "df = df.fillna('')\n",
    "df = df.astype('str')\n",
    "df['Text'] = df.apply(lambda x: x['Subject'] + ', ' + x['Message'], axis=1)\n",
    "df['Label'] = np.where(df['Spam/Ham'].values == 'ham', 0, 1)\n",
    "df['Text'] = df['Text'].progress_apply(tp.normalize_text)\n",
    "\n",
    "count_vectorizer = ft.CountVectorizer()\n",
    "count_vectorizer.fit(df['Text'])\n",
    "vocabulary = count_vectorizer.get_feature_names_out()\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_renames = {\n",
    "    'shap': 'SHAP',\n",
    "    'term_strength': 'TS',\n",
    "    'trl': 'TRL',\n",
    "    'eccd': 'ECCD',\n",
    "    'mutual_information': 'MI',\n",
    "    'chi2': 'chi2',\n",
    "    'tfidf': 'TF-IDF',\n",
    "    'linear_measure_5': 'LM',\n",
    "    'lfs': 'LFS'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TRAIN = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extractor_timings(files: dict) -> pd.DataFrame:\n",
    "    methods = files.keys()\n",
    "    df = pd.DataFrame(columns=methods)\n",
    "    n_words_list = set()\n",
    "    method_list = set()\n",
    "    for method, words_dict in files.items():\n",
    "        method_list.add(method)\n",
    "        for n_words, info in words_dict.items():\n",
    "                df.loc[n_words, method] = info['timing']['extractor_fit']\n",
    "                n_words_list.add(n_words)\n",
    "                \n",
    "    df.index = df.index.astype('int')\n",
    "    df = df.sort_index()\n",
    "    df = df.apply(pd.to_timedelta)\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].dt.total_seconds()\n",
    "    return df, method_list, n_words_list\n",
    "\n",
    "def plot_extractors_timings(df) -> plt.figure:\n",
    "    df = df.copy(deep=True)\n",
    "    df = df.rename(columns=methods_renames)\n",
    "    axes = df.plot(logy=True, logx=True, figsize=(16, 10))\n",
    "    axes.set_xlabel('# selected words')\n",
    "    axes.set_ylabel('runtime (sec)')\n",
    "    # axes.set_label('runtime (sec)')\n",
    "    axes.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.title('Feature extractor runtimes vs number of the selected words.')\n",
    "    plt.savefig('../figures/enron/enron_extractor_runtime_vs_n_words.pdf', bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def get_selected_words_per_extractor_per_n_words(files: dict, vocabulary, n_words_list, method_list):\n",
    "    df_dict = {}\n",
    "\n",
    "    for n_words in n_words_list:\n",
    "        df_dict[n_words] = pd.DataFrame(index=vocabulary, columns=list(method_list)).fillna(0)\n",
    "\n",
    "    for method, words_dict in files.items():\n",
    "        for n_words, info in words_dict.items():\n",
    "            df_dict[n_words].loc[info['selected_vocabulary'][0], method] = 1\n",
    "    return df_dict\n",
    "\n",
    "def get_cross_jaccard_score(df):\n",
    "    methods = df.columns.tolist()\n",
    "    jaccard_df = pd.DataFrame(index=methods, columns=methods)\n",
    "    for method_1 in methods:\n",
    "        for method_2 in methods:\n",
    "            jaccard_df.loc[method_1, method_2] = jaccard_score(df[method_1], df[method_2])\n",
    "    return jaccard_df\n",
    "\n",
    "def get_similarity_metrics(df_dict):\n",
    "    correlations_dict = {}\n",
    "    jaccard_score_dict = {}\n",
    "    for n_words, df in df_dict.items():\n",
    "        df_filtered = df.loc[:,(df.sum(axis=0) != 0).values] # remove lfs (or other methods) when they have no values\n",
    "        correlations_dict[n_words] = df_filtered.corr() \n",
    "        jaccard_score_dict[n_words] = get_cross_jaccard_score(df_filtered)\n",
    "    \n",
    "    return correlations_dict, jaccard_score_dict\n",
    "\n",
    "def compare_shap_over_n_words_set_similarity(df_dict: dict, n_words_list, method_list):\n",
    "    df_comp = pd.DataFrame(columns=method_list, index=n_words_list)\n",
    "    for n_words, df in df_dict.items():\n",
    "        cols = df.columns\n",
    "        df_comp.loc[n_words, cols] = df['shap'][cols]\n",
    "    df_comp.index = df_comp.index.astype('int')\n",
    "    df_comp = df_comp.sort_index()\n",
    "    df_comp = df_comp.drop(columns=['shap'])\n",
    "    return df_comp\n",
    "\n",
    "def compare_methods_set_similarity(df_dict: dict, n_words_list, method_list):\n",
    "    df_comp = pd.DataFrame(columns=method_list, index=n_words_list)\n",
    "    for n_words, df in df_dict.items():\n",
    "        cols = df.columns\n",
    "        df_comp.loc[n_words, cols] = df['shap'][cols]\n",
    "    df_comp.index = df_comp.index.astype('int')\n",
    "    df_comp = df_comp.sort_index()\n",
    "    df_comp = df_comp.drop(columns=['shap'])\n",
    "    return df_comp\n",
    "\n",
    "def compare_performance_over_n_words(files, n_words_list, method_list, baseline=None):\n",
    "    metrics = ['precision', 'recall', 'f1-score']\n",
    "    cols = []\n",
    "    for method in method_list:\n",
    "        for metric in metrics:\n",
    "            cols.append((method, f'{metric}_mean'))\n",
    "            cols.append((method, f'{metric}_std'))\n",
    "    df_metrics = pd.DataFrame(columns=pd.MultiIndex.from_tuples(cols), index=n_words_list)\n",
    "\n",
    "    for method, words_dict in files.items():\n",
    "        print(method)\n",
    "        for n_words, info in words_dict.items():\n",
    "            for metric in metrics:\n",
    "                df_metrics.loc[n_words, (method, f'{metric}_mean')] = info[f'classification_report_{TEST_TRAIN}']['macro avg'][f'{metric}_mean']\n",
    "                df_metrics.loc[n_words, (method, f'{metric}_std')] = info[f'classification_report_{TEST_TRAIN}']['macro avg'][f'{metric}_std']\n",
    "    if baseline is not None:\n",
    "        for n_words in n_words_list:\n",
    "            for metric in metrics:\n",
    "                df_metrics.loc[n_words, ('baseline',f'{metric}_mean')] = baseline['macro avg'][f'{metric}_mean']\n",
    "                df_metrics.loc[n_words, ('baseline',f'{metric}_std')] = baseline['macro avg'][f'{metric}_std']\n",
    "\n",
    "    df_metrics.index = df_metrics.index.astype('int')\n",
    "    df_metrics = df_metrics.sort_index()\n",
    "    return df_metrics\n",
    "\n",
    "def plot_performance_metrics(df_metrics, metrics_to_plot, methods_to_plot):\n",
    "    for metric in metrics_to_plot:\n",
    "        fig, ax = plt.subplots(figsize=(9,5))\n",
    "\n",
    "        for method in methods_to_plot:\n",
    "            ax.errorbar(x=df_metrics.index, y=df_metrics.loc[:, (method, f'{metric}_mean')], yerr=df_metrics.loc[:, (method, f'{metric}_std')], label=method, alpha=0.5)\n",
    "        fig.tight_layout()\n",
    "        ax.set_xlabel('# of kept words')\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_ylabel(f'{metric}')\n",
    "        ax.legend(loc=\"lower right\")\n",
    "        # plt.title(f'Enron corpus {metric} vs # of kept words.')\n",
    "        plt.savefig(f'../figures/enron/enron_perf_{metric}_train.pdf', bbox_inches=\"tight\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, method_list, n_words_list = get_extractor_timings(files)\n",
    "df.to_csv('../figures/csv_outputs/timings/enron_n_word_timings.csv', sep=';')\n",
    "plot_extractors_timings(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = get_selected_words_per_extractor_per_n_words(files, vocabulary, n_words_list, method_list)\n",
    "correlations_dict, jaccard_score_dict = get_similarity_metrics(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap_correlations = compare_shap_over_n_words_set_similarity(correlations_dict, n_words_list, method_list)\n",
    "shap_jaccard = compare_shap_over_n_words_set_similarity(jaccard_score_dict, n_words_list, method_list)\n",
    "# shap_correlations.to_csv('../results/tables/enron_correlations.csv', sep=';')\n",
    "shap_jaccard.to_csv(f'../figures/csv_outputs/jaccard/enron_jaccard_{TEST_TRAIN}.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/enron_baseline.json', 'r') as file:\n",
    "    baseline_enron = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shap\n",
      "term_strength\n",
      "trl\n",
      "eccd\n",
      "mutual_information\n",
      "chi2\n",
      "tfidf\n",
      "linear_measure_5\n"
     ]
    }
   ],
   "source": [
    "df_metrics = compare_performance_over_n_words(files, n_words_list, method_list, baseline=baseline_enron[f'classification_report_{TEST_TRAIN}'])\n",
    "df_metrics.to_csv(f'../figures/csv_outputs/metrics/enron_performance_{TEST_TRAIN}.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = list(method_list)[:]\n",
    "methods.append('baseline')\n",
    "plot_performance_metrics(df_metrics, ['recall', 'precision', 'f1-score'], methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_table = pd.read_csv('../results/tables/enron_jaccard.csv', sep=';', index_col=0)\n",
    "jaccard_table.index.name = 'n_words'\n",
    "# jaccard_table = jaccard_table.drop(columns='lfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\centering\n",
      "\\caption{Jaccard score between Shapley values based method and the rest.}\n",
      "\\begin{tabular}{lrrrrrrr}\n",
      "\\toprule\n",
      "{} &  eccd &  mutual\\_information &   trl &  linear\\_measure\\_5 &  term\\_strength &  tfidf &  chi2 \\\\\n",
      "n\\_words &       &                     &       &                   &                &        &       \\\\\n",
      "\\midrule\n",
      "50      & 0.205 &               0.351 & 0.075 &             0.235 &          0.220 &  0.266 & 0.299 \\\\\n",
      "100     & 0.235 &               0.351 & 0.047 &             0.282 &          0.266 &  0.282 & 0.351 \\\\\n",
      "200     & 0.286 &               0.394 & 0.042 &             0.356 &          0.338 &  0.270 & 0.375 \\\\\n",
      "500     & 0.247 &               0.344 & 0.018 &             0.445 &          0.422 &  0.357 & 0.350 \\\\\n",
      "1000    & 0.214 &               0.299 & 0.010 &             0.505 &          0.499 &  0.463 & 0.305 \\\\\n",
      "3000    & 0.099 &               0.132 & 0.016 &             0.192 &          0.191 &  0.184 & 0.137 \\\\\n",
      "5000    & 0.082 &               0.100 & 0.024 &             0.125 &          0.125 &  0.121 & 0.102 \\\\\n",
      "10000   & 0.070 &               0.076 & 0.042 &             0.085 &          0.085 &  0.085 & 0.077 \\\\\n",
      "15000   & 0.082 &               0.086 & 0.037 &             0.092 &          0.092 &  0.092 & 0.087 \\\\\n",
      "25000   & 0.112 &               0.113 & 0.110 &             0.113 &          0.113 &  0.114 & 0.113 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.float_format', '{:,.3f}'.format):\n",
    "    print(jaccard_table.to_latex(caption='Jaccard score between Shapley values based method and the rest.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\centering\n",
      "\\caption{Jaccard score between the methods at 3000 kept words.}\n",
      "\\begin{tabular}{lllllllll}\n",
      "\\toprule\n",
      "{} &  eccd &  shap & mutual\\_information &   trl & linear\\_measure\\_5 & term\\_strength & tfidf &  chi2 \\\\\n",
      "\\midrule\n",
      "eccd               & 1.000 & 0.099 &              0.757 & 0.001 &            0.354 &         0.338 & 0.360 & 0.679 \\\\\n",
      "shap               & 0.099 & 1.000 &              0.132 & 0.016 &            0.192 &         0.191 & 0.184 & 0.137 \\\\\n",
      "mutual\\_information & 0.757 & 0.132 &              1.000 & 0.003 &            0.483 &         0.463 & 0.475 & 0.895 \\\\\n",
      "trl                & 0.001 & 0.016 &              0.003 & 1.000 &            0.011 &         0.011 & 0.009 & 0.003 \\\\\n",
      "linear\\_measure\\_5   & 0.354 & 0.192 &              0.483 & 0.011 &            1.000 &         0.963 & 0.815 & 0.519 \\\\\n",
      "term\\_strength      & 0.338 & 0.191 &              0.463 & 0.011 &            0.963 &         1.000 & 0.816 & 0.499 \\\\\n",
      "tfidf              & 0.360 & 0.184 &              0.475 & 0.009 &            0.815 &         0.816 & 1.000 & 0.505 \\\\\n",
      "chi2               & 0.679 & 0.137 &              0.895 & 0.003 &            0.519 &         0.499 & 0.505 & 1.000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jaccard_3000_df = jaccard_score_dict['3000']\n",
    "with pd.option_context('display.float_format', '{:,.3f}'.format):\n",
    "    print(jaccard_3000_df.to_latex(caption='Jaccard score between the methods at 3000 kept words.'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
